"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ formal/informal.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –¢–ï–•–ù–ò–ö–ò:
1. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ (length normalization)
2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ (noise injection)
3. –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏

–ë–µ–∑ —ç—Ç–∏—Ö —Ç–µ—Ö–Ω–∏–∫ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏—á—å "–∏–¥–µ–∞–ª—å–Ω–æ–π" —Ç–æ—á–Ω–æ—Å—Ç–∏,
–æ–±—É—á–∏–≤—à–∏—Å—å –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è—Ö –≤ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤, –∞ –Ω–µ –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏!
"""

import os
import re
import random
import argparse
import pandas as pd
import numpy as np
from typing import List, Tuple

try:
    from datasets import load_dataset
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("‚ö†Ô∏è datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. pip install datasets")

SEED = 42
random.seed(SEED)
np.random.seed(SEED)


def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏"""
    text = str(text).strip()
    text = re.sub(r"http\\S+|www\\S+", "", text)
    text = re.sub(r"\\S+@\\S+", "", text)
    text = re.sub(r"[@#]\\w+", "", text)
    text = re.sub(r"\\s+", " ", text)
    text = re.sub(r"([!?.]){2,}", r"\\1", text)
    return text.strip()


def is_valid_text(text: str, min_words: int = 4, max_words: int = 100) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return False
    words = text.split()
    if len(words) < min_words or len(words) > max_words:
        return False
    if not re.search(r"[–∞-—è—ë–ê-–Ø–Å]", text):
        return False
    return True


def normalize_lengths(
    formal_texts: List[str],
    informal_texts: List[str],
    target_range: Tuple[int, int] = (8, 40),
) -> Tuple[List[str], List[str]]:
    """–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏."""
    min_words, max_words = target_range

    formal_filtered = [
        t for t in formal_texts if min_words <= len(t.split()) <= max_words
    ]
    informal_filtered = [
        t for t in informal_texts if min_words <= len(t.split()) <= max_words
    ]

    print(f"\nüìè –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã ({min_words}-{max_words} —Å–ª–æ–≤):")
    print(f"   Formal: {len(formal_texts)} ‚Üí {len(formal_filtered)}")
    print(f"   Informal: {len(informal_texts)} ‚Üí {len(informal_filtered)}")

    def bin_by_length(texts: List[str], n_bins: int = 10) -> dict:
        bins = {}
        for t in texts:
            word_count = len(t.split())
            bin_idx = min(word_count // 5, n_bins - 1)
            bins.setdefault(bin_idx, []).append(t)
        return bins

    formal_bins = bin_by_length(formal_filtered)
    informal_bins = bin_by_length(informal_filtered)

    formal_balanced = []
    informal_balanced = []
    all_bins = set(formal_bins.keys()) | set(informal_bins.keys())

    for bin_idx in all_bins:
        f_texts = formal_bins.get(bin_idx, [])
        i_texts = informal_bins.get(bin_idx, [])
        if f_texts and i_texts:
            n = min(len(f_texts), len(i_texts))
            formal_balanced.extend(random.sample(f_texts, n))
            informal_balanced.extend(random.sample(i_texts, n))

    print(f"   –ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {len(formal_balanced)}, {len(informal_balanced)}")
    return formal_balanced, informal_balanced


def add_noise(text: str, noise_prob: float = 0.1) -> str:
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è."""
    if random.random() > noise_prob:
        return text

    words = text.split()
    if len(words) < 3:
        return text

    noise_type = random.choice(["drop", "case", "swap", "typo"])

    if noise_type == "drop" and len(words) > 4:
        idx = random.randint(1, len(words) - 2)
        words.pop(idx)
    elif noise_type == "case":
        idx = random.randint(0, len(words) - 1)
        words[idx] = words[idx].lower() if random.random() > 0.5 else words[idx].upper()
    elif noise_type == "swap" and len(words) > 2:
        idx = random.randint(0, len(words) - 2)
        words[idx], words[idx + 1] = words[idx + 1], words[idx]
    elif noise_type == "typo":
        idx = random.randint(0, len(words) - 1)
        word = words[idx]
        if len(word) > 3:
            char_idx = random.randint(1, len(word) - 2)
            if random.random() > 0.5:
                word = word[:char_idx] + word[char_idx] + word[char_idx:]
            else:
                word = word[:char_idx] + word[char_idx + 1:]
            words[idx] = word

    return " ".join(words)


def augment_with_noise(texts: List[str], augment_factor: float = 0.3) -> List[str]:
    """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∑–∞—à—É–º–ª–µ–Ω–∏—è"""
    augmented = list(texts)
    n_augment = int(len(texts) * augment_factor)

    for _ in range(n_augment):
        original = random.choice(texts)
        noisy = add_noise(original, noise_prob=0.8)
        augmented.append(noisy)

    print(f"   –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {len(texts)} ‚Üí {len(augmented)}")
    return augmented


def load_formal_data(max_samples: int = 5000) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–Ω–æ–≤–æ—Å—Ç–∏)"""
    texts = []
    if not HF_AVAILABLE:
        return texts

    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ FORMAL –¥–∞–Ω–Ω—ã—Ö...")
    try:
        print("   Lenta.ru...")
        lenta = load_dataset("IlyaGusev/lenta-ru-news", split="train", trust_remote_code=True)
        for item in list(lenta)[: max_samples * 2]:
            text = item.get("text", "")
            if not text:
                continue
            sentences = re.split(r"(?<=[.!?])\\s+", text)
            short_text = " ".join(sentences[:2]).strip()
            if short_text and not short_text.endswith((".", "!", "?")):
                short_text += "."
            short_text = clean_text(short_text)
            if is_valid_text(short_text):
                texts.append(short_text)
        print(f"   ‚úÖ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

    return list(set(texts))[:max_samples]


def load_informal_data(max_samples: int = 5000) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (—Å–æ—Ü—Å–µ—Ç–∏)"""
    texts = []
    if not HF_AVAILABLE:
        return texts

    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ INFORMAL –¥–∞–Ω–Ω—ã—Ö...")
    try:
        print("   RuSentiment (VK)...")
        rusentiment = load_dataset("RuSentiment/rusentiment", split="train", trust_remote_code=True)
        for item in rusentiment:
            text = item.get("text", "")
            text = clean_text(text)
            if is_valid_text(text, min_words=3, max_words=50):
                texts.append(text)
        print(f"   ‚úÖ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

    return list(set(texts))[:max_samples]


FORMAL_TEMPLATES = [
    "–£–≤–∞–∂–∞–µ–º—ã–µ –∫–æ–ª–ª–µ–≥–∏, –Ω–∞–ø—Ä–∞–≤–ª—è—é –≤–∞–º –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∑–∞ —Ç–µ–∫—É—â–∏–π –ø–µ—Ä–∏–æ–¥.",
    "–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã–º–∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—è–º–∏, –ø—Ä–æ—à—É –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –≤—Å—Ç—Ä–µ—á–µ.",
    "–ù–∞—Å—Ç–æ—è—â–∏–º —É–≤–µ–¥–æ–º–ª—è–µ–º –≤–∞—Å –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
    "–ë–ª–∞–≥–æ–¥–∞—Ä–∏–º –∑–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ –∏ –Ω–∞–¥–µ–µ–º—Å—è –Ω–∞ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ.",
    "–ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è –≤–∞—à–µ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è —Å–æ–æ–±—â–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–µ.",
    "–ü—Ä–æ—Å–∏–º –ø—Ä–∏–Ω—è—Ç—å –∫ —Å–≤–µ–¥–µ–Ω–∏—é –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ–∏–∫–µ —Ä–∞–±–æ—Ç—ã –æ—Ç–¥–µ–ª–∞.",
    "–í —Å–≤—è–∑–∏ —Å –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –ø–µ—Ä–µ–Ω–æ—Å–∏–º —Å–æ–≤–µ—â–∞–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –Ω–µ–¥–µ–ª—é.",
    "–ù–∞–ø—Ä–∞–≤–ª—è—é –Ω–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø—Ä–∞–≤–∫–∞–º–∏.",
    "–ü–æ –¥–∞–Ω–Ω—ã–º –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã, –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ —Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞.",
    "–°–æ–≥–ª–∞—Å–Ω–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ, –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ –ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤.",
]

INFORMAL_TEMPLATES = [
    "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —Å–∞–º? –î–∞–≤–Ω–æ –Ω–µ –≤–∏–¥–µ–ª–∏—Å—å, —Å–æ—Å–∫—É—á–∏–ª—Å—è —É–∂–µ!",
    "–ó–¥–∞—Ä–æ–≤–∞! –ß—ë –¥–µ–ª–∞–µ—à—å —Å–µ–≥–æ–¥–Ω—è –≤–µ—á–µ—Ä–æ–º? –ú–æ–∂–µ—Ç –ø–æ–≥—É–ª—è–µ–º?",
    "–û–π, –ø—Ä–∏–≤–µ—Ç! –°–ª—É—à–∞–π, —Ö–æ—Ç–µ–ª–∞ —Å–ø—Ä–æ—Å–∏—Ç—å —Ç–µ–±—è –∫–æ–µ –æ —á—ë–º.",
    "–•–µ–π! –¢—ã –≤–∏–¥–µ–ª —á—Ç–æ –≤—á–µ—Ä–∞ –±—ã–ª–æ? –í–æ–æ–±—â–µ –∂–µ—Å—Ç—å –∫–∞–∫–∞—è-—Ç–æ!",
    "–ê—Ö–∞—Ö–∞—Ö, —ç—Ç–æ –±—ã–ª–æ —Ç–∞–∫ —Å–º–µ—à–Ω–æ, —è —á—É—Ç—å –Ω–µ —É–º–µ—Ä —Å–æ —Å–º–µ—Ö—É!",
    "–ë–ª–∏–Ω, –Ω—É –≤–æ—Ç –æ–ø—è—Ç—å! –î–æ—Å—Ç–∞–ª–æ —É–∂–µ —ç—Ç–æ –≤—Å—ë, —á–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ.",
    "–û—Ñ–∏–≥–µ—Ç—å! –°–µ—Ä—å—ë–∑–Ω–æ? –ù–µ –º–æ–≥—É –ø–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞!",
    "–ö–∞–ø–µ—Ü –∫–∞–∫–æ–π-—Ç–æ, —É –º–µ–Ω—è –ø—Ä–æ—Å—Ç–æ —Å–ª–æ–≤ –Ω–µ—Ç –æ—Ç —ç—Ç–æ–≥–æ –≤—Å–µ–≥–æ.",
    "–°–ª—É—à–∞–π, –º–æ–∂–µ—à—å —Å–∫–∏–Ω—É—Ç—å —Ç–µ —Ñ–æ—Ç–∫–∏ —Å –≤—ã—Ö–æ–¥–Ω—ã—Ö? –ü–ª–∏–∑!",
    "–ë—Ä–∞—Ç–∞–Ω, –≤—ã—Ä—É—á–∞–π, —Å—Ä–æ—á–Ω–æ –Ω—É–∂–Ω–∞ —Ç–≤–æ—è –ø–æ–º–æ—â—å —Å –æ–¥–Ω–∏–º –¥–µ–ª–æ–º.",
]


def generate_from_templates(templates: List[str], n_samples: int = 1000) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ —à–∞–±–ª–æ–Ω–æ–≤ —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏"""
    result = []
    n_per_template = n_samples // len(templates) + 1

    for template in templates:
        for _ in range(n_per_template):
            text = template
            if random.random() > 0.5:
                text = text.lower()
            if random.random() > 0.7:
                text = text.replace("!", ".")
            if random.random() > 0.8:
                text = add_noise(text, noise_prob=1.0)
            result.append(text)

    return result[:n_samples]


def prepare_dataset(
    output_path: str = "train.csv",
    n_samples: int = 3000,
    use_noise: bool = True,
    length_range: Tuple[int, int] = (8, 40),
) -> pd.DataFrame:
    """–ü–æ–ª–Ω—ã–π pipeline –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    print("=" * 60)
    print("üöÄ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
    print("=" * 60)

    formal_texts = load_formal_data(max_samples=n_samples * 2)
    informal_texts = load_informal_data(max_samples=n_samples * 2)

    MIN_SAMPLES = 500
    if len(formal_texts) < MIN_SAMPLES:
        print("\n‚ö†Ô∏è –î–æ–±–∞–≤–ª—è–µ–º formal —à–∞–±–ª–æ–Ω—ã...")
        formal_texts.extend(generate_from_templates(FORMAL_TEMPLATES, MIN_SAMPLES))
    if len(informal_texts) < MIN_SAMPLES:
        print("\n‚ö†Ô∏è –î–æ–±–∞–≤–ª—è–µ–º informal —à–∞–±–ª–æ–Ω—ã...")
        informal_texts.extend(generate_from_templates(INFORMAL_TEMPLATES, MIN_SAMPLES))

    formal_texts, informal_texts = normalize_lengths(
        formal_texts, informal_texts, target_range=length_range
    )

    if use_noise:
        print("\nüîä –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è:")
        formal_texts = augment_with_noise(formal_texts, augment_factor=0.2)
        informal_texts = augment_with_noise(informal_texts, augment_factor=0.2)

    min_size = min(len(formal_texts), len(informal_texts))
    n_final = min(min_size, n_samples)

    formal_sample = random.sample(formal_texts, n_final)
    informal_sample = random.sample(informal_texts, n_final)

    formal_data = [{"text": t, "label": "formal"} for t in formal_sample]
    informal_data = [{"text": t, "label": "informal"} for t in informal_sample]

    df = pd.DataFrame(formal_data + informal_data)
    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)

    print("\n" + "=" * 60)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 60)
    print(f"–í—Å–µ–≥–æ: {len(df)}")
    print(f"–ö–ª–∞—Å—Å—ã: {df['label'].value_counts().to_dict()}")

    df["word_count"] = df["text"].str.split().str.len()
    print("\n–î–ª–∏–Ω–∞ (—Å–ª–æ–≤–∞):")
    print(df.groupby("label")["word_count"].agg(["mean", "std"]).round(1))

    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
    df[["text", "label"]].to_csv(output_path, index=False)
    print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")

    return df


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--output", "-o", default="train.csv")
    parser.add_argument("--samples", "-n", type=int, default=3000)
    parser.add_argument("--min-words", type=int, default=8)
    parser.add_argument("--max-words", type=int, default=40)
    parser.add_argument("--no-noise", action="store_true")

    args = parser.parse_args()

    prepare_dataset(
        output_path=args.output,
        n_samples=args.samples,
        use_noise=not args.no_noise,
        length_range=(args.min_words, args.max_words),
    )


if __name__ == "__main__":
    main()
